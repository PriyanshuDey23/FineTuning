# -*- coding: utf-8 -*-
"""Finetune_model_testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11MB4_G3SPF4f_p21tJLqDClrOskPigJk
"""

!pip install unsloth # install unsloth

# hf_token="" # not token required as my repo is public

## check hugging face token permission
# from huggingface_hub import whoami
# whoami(token=hf_token)

from unsloth import FastLanguageModel

BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
LORA_REPO  = "babai2003/deepseek-r1-medical-lora"

# Load base model
base_model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    load_in_4bit=True
)

# Make a copy for LoRA fine-tuned version
# Note: just a reference; you can also deep copy if needed
model_lora = base_model

# Load your LoRA adapter
model_lora.load_adapter(LORA_REPO)

# Enable inference mode on LoRA model
FastLanguageModel.for_inference(model_lora)

# âœ… Now you have two separate references:
# base_model -> untouched original
# model_lora -> LoRA fine-tuned model

print(model_lora.active_adapters)

# here is the proof that i am using mine:-(4bit)

# (q_proj): lora.Linear4bit(
#   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
#   (lora_dropout): ModuleDict(
#     (default): Identity()

from huggingface_hub import hf_hub_download

# Download the README.md file of your LoRA repo
readme_path = hf_hub_download(
    repo_id="babai2003/deepseek-r1-medical-lora",
    filename="README.md",
    # token="hf_XXXX..."  # your HF token
)

# Read and print
with open(readme_path, "r") as f:
    content = f.read()

print(content)

# independent
question = """A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,
and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive,
catalase-negative, gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.
What is the most likely predisposing factor for this patient's condition?
"""

prompt = """
You are a medical expert.

Question:
{question}

Answer:
"""

# Make sure inference mode is set
FastLanguageModel.for_inference(model_lora)

# Tokenize
inputs = tokenizer(
    [prompt.format(question=question)],
    return_tensors="pt"
).to("cuda")

# Generate
outputs = model_lora.generate(
    **inputs,
    max_new_tokens = 300,
    temperature = 0.7,
    top_p = 0.9,
    do_sample = True,
    use_cache = True,
)

# Decode
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Extract answer safely
answer = response.split("Answer:")[-1].strip()
print(answer)

# from colab before saving the file i have used it
question = """A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,
              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,
              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.
              What is the most likely predisposing factor for this patient's condition?"""


prompt_style = """
Below is a task description along with additional context provided in the input section. Your goal is to provide an accurate, clinically sound response that directly addresses the request.

Use your medical expertise to ensure the answer is evidence-based, clearly structured, and easy to understand. When appropriate, briefly explain the rationale behind key conclusions without revealing internal reasoning processes.

### Task:
You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.
Provide a clear, concise, and clinically appropriate response. Include relevant explanations, differential considerations, and recommendations as needed.

### Query:
{question}

### Answer:

"""

FastLanguageModel.for_inference(model_lora)

# Tokenize the input
inputs = tokenizer([prompt_style.format(question=question)], return_tensors="pt").to("cuda")  # tokenize the input

# Generate a response
outputs = model_lora.generate (
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

# Decode the response tokens back to text
response = tokenizer.batch_decode(outputs)

print(response)

print(response[0].split("### Answer:")[1])